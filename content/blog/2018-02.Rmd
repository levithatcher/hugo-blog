---
title: "Studying 20th century SLC climate change"
excerpt: "How has the winter changed in our lifetime?"
date: '2018-02-09'
---

## Background

For those folks that have spent most of their lives in SLC, they might have noticed that snow used to stay on the ground all winter, but now is gone within a few days of a storm. You hear folks mention this occasionally in conversation. Let's get some data to check out what's actually happened.

We'll pull data (via [NOAA](https://www.ncdc.noaa.gov/cdo-web/datatools/findstation) from both urban and remote sites in the vicinity of SLC.

## Grab the data

```{r}
library(tidyverse)
d <- read_csv('../data/SLC_Min_Temp_1948-2007.csv')

# Look at the data
head(d)
```

Nice, so the data is in a tidy format, which here means that multiple sites are handled under the NAME column. This means that if we grabbed data from more sites in the future, our processes here would be reproducible without modifications.

## Initial analysis

### Check what's missing

Let's check on how much of the daily data is missing.

```{r}
library(lubridate)
d %>%
  mutate(Year = year(DATE)) %>% 
  filter(Year > 2000 & Year < 2005) %>% 
  group_by(Year, NAME) %>% 
  summarize(Count = n())
```

Looks like the Triad Center site, which is in the [CBD](https://en.wikipedia.org/wiki/Central_business_district) of SLC, had some serious collection issues in the early 2000s (such that entire years are missing). Let's proceed on with data from the SLC Airport and the Cedar Mountains, since they're populated fairly faithfully. Note that the Cedar Mountains site is at lat/lon 40.3008/-112.7764 and as the crow flies is 55 miles southwest SLC's [CBD](https://en.wikipedia.org/wiki/Central_business_district).

### Prep the data

```{r}
d <- d %>%
  filter(NAME != 'SALT LAKE TRIAD CENTER, UT US') %>% 
  # Rename the sites and NAME column for readability
  mutate(NAME = ifelse(NAME == 'SALT LAKE CITY INTERNATIONAL AIRPORT, UT US',
                       'SLC Intl',
                       'Cedar Mtns')) %>% 
  rename(Site = NAME, Date = DATE, TMin = TMIN) %>% 
  select(Site, Date, TMin)
```


### Explore the data

Let's first look at the smoothed average over the last 30 years.

```{r}
library(scales)

d %>% 
  filter(year(Date) > 1987) %>% 
ggplot(., aes(x = Date, y = TMin, color = Site)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = 'loess') +
  labs(title = 'SLC area daily min temperature from 1988-2017') +
  scale_x_date(name = "Year",
               labels = date_format("%Y")) +
  scale_y_continuous(name = "Minimium temperature",
                     breaks = c(-20,-10,0,10,20,30,40,50,60,70,80),
                     limits = c(-25,85))
```

By and large this is what we'd expect for SLC-area lows--summertime lows only occasionally get into the 70s and winter lows only rarely get below zero. This upward trend confirms what long-time residents have anecdotally expressed about a decrease in quasi-permanent winter ground snow.

Note that SLC Intl has risen more than Cedar Mtns, but both show a noticeable increase in daily temperature minima over the last 30 years. For curiosity's sake, let's grab a longer time series for SLC Intl.

Note that I'll use a linear smoother here (instead of loess) since in a longer time series we're naturally less curious about short-term variation.

```{r}
d %>% 
  filter(Site == 'SLC Intl') %>%
ggplot(., aes(x = Date, y = TMin)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = 'lm') +
  labs(title = 'SLC Intl daily min temperature from 1948-2017') +
  scale_x_date(name = "Year",
               labels = date_format("%Y")) +
  scale_y_continuous(name = "Minimium temperature",
                     breaks = c(-20,-10,0,10,20,30,40,50,60,70,80),
                     limits = c(-25,85))

```

This is over-plotted, but it's not difficult to see that the trend line goes from ~38°F to ~44°F, which is surprisingly high compared with the [global temperature rise of 1.69°F since 1880](https://www.climate.gov/news-features/understanding-climate/climate-change-global-temperature). Since SLC has seen a lot of urban growth over the last 70 years, a heat-island effect may be playing a part of this (which is why we grabbed data from the remote Cedar Mtns site). 

## Putting rigor into trend confirmation

Now that we've plotted the data and it roughly looks correct, let's use stats to get at whether the trend is significant over the long haul and then over shorter time frames. Note that this is intended as more of an exploration of resampling methods rather than a contribution to climate change research.

### Comparing average lows by decade?

Let's now take the sum for each decade from 1950-2010 and see how they've changed. While we'd expect them to be quite close, plotting 95% confidence intervals around the points will help us get a sense of the certainty around each mean.

Note that we calculate the means per decade with a naive 95% confidence interval. Can you catch what's naive about it?

```{r}
d_slim <- d %>% 
  filter(Site == 'SLC Intl' & year(Date) > 1949) %>%
  mutate(Decade = year(Date) - (year(Date) %% 10)) %>% 
  group_by(Decade) %>%
  mutate(Mean = base::mean(TMin, na.rm = TRUE),
         SD = stats::sd(TMin, na.rm = TRUE),
         StdError = SD / sqrt(n()),
         CI = 1.96 * StdError)
  

ggplot(d_slim, aes(y = TMin, x = Decade)) +
  #geom_point() +
  #stat_summary(fun.data = mean_cl_normal, conf.int = 0.95, geom = "errorbar") +
  stat_summary(fun.y = mean, geom = "point") +
  geom_errorbar(aes(ymin = Mean - CI, ymax = Mean + CI)) +
  scale_x_continuous(breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010)) +
  scale_y_continuous(breaks = c(36,37,38,39,40,41,42,43,44,45)) +
  coord_cartesian(ylim = c(36,45))
```
The error bars here are too small (hence the naivety) because of the autocorrelation in the dataset. Backing up a bit, since the temperature tomorrow is somewhat explained by the temperature yesterday, there's some autocorrelation that effectively reduces the degrees of freedom in the data. That means that things like standard errors and t-tests need to be adjusted (so that we don't mistakenly reject the null hypothesis).

### Adjusting error bars for autocorrelation

A simple way to think about this, is that we need an adjustment factor for our standard error (since that's what confidence intervals are often based on). One can do that [via an autocorrelation coefficient](https://en.wikipedia.org/wiki/Standard_error#Correction_for_correlation_in_the_sample). But what's the easiest way to find that? If you had an AR(1) process (where you only had a one-step dependence in your data) then you could use the [Prais-Winsten estimate](https://en.wikipedia.org/wiki/Prais%E2%80%93Winsten_estimation). Since temperature data has much more of a memory, we need a different route.

Note that to get at autocorrelation, one has to first remove the trend. We can do this via the `stl` function.

```{r}
measurement_ts <- ts(d_slim$TMin, 
                     start=c(1950,1,1), 
                     frequency = 365.25)

# Perform decomposition into trend, seasonal, etc
stl_slim <- stats::stl(x = measurement_ts, 
                       s.window = "periodic")

# Put de-trend data back into tibble
d_slim$DeTrendedTMin <- stl_slim$time.series[,'seasonal']
```

Now that we've de-trended the data, let's calculate the autocorrelation coefficient.


```{r}

res <- stats::acf(x = d_slim$DeTrendedTMin, type = "correlation")

mean_auto_corr <- mean(res$acf)

```



















```{r}
library(boot)



# ts_fun <- function(vect) {

# r <- d %>%
#   mutate(Decade = year(Date) - year(Date) %% 10) %>% 
#   #group_by(Decade) %>% 
#   select(TMin) %>% 
#   #do(meany = mean(.$TMin, na.rm = TRUE)) 
#   do(compute_block_boot(.$TMin))

  
```


# d_small <- d %>%
#   # Create decade column using modulus (ie, remainders)
#   mutate(Decade = year(Date) - year(Date) %% 10) %>% 
#   group_by(Decade) %>%
#   broom::bootstrap(10, by_group = TRUE) %>% 
#   do(tidy(mean(TMin)))
```

```{r}
# res <- d %>%
#  mutate(Decade = year(Date) - year(Date) %% 10) %>%
#  group_by(Decade) %>%
#  select(TMin) %>%
#  modelr::bootstrap(100) %>%
#  mutate(sepal_mean = map_dbl(strap, ~mean(as_data_frame(.x)$TMin)))
#res
```


```{r}
# plot(out, index=1)
# ggplot(data = out, aes(x = out$t[,1])) +
#   geom_point()
```




