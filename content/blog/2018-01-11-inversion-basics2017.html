---
title: "SLC Air Quality - Five Years On"
excerpt: "Are we doing better or worse?"
date: '2018-01-11'
draft: false
showpagemeta: true
---



<section id="background-on-slc-air" class="level2">
<h2>Background on SLC Air</h2>
<p>For those unfamiliar, Salt Lake City is famous for its exquisite natural beauty, and its winter temperature inversions. These events trap emissions in the valley and make life unpleasant. For the uninitiated, see <a href="http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html">here</a> and <a href="https://ehp.niehs.nih.gov/1104349/">here</a>. The focus of <em>today’s</em> post will be digging into five years of <strong>PM2.5</strong> data in SLC, as this type of inversion-exacerbated pollutant is most frequently linked to <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4740125/">negative health outcomes</a>. Check out this episode from 2016 (image from <a href="http://health.utah.gov/utahair/SLC_51ug-m3.jpg">here</a>):</p>
<p><img src="figs/2017-01-11-inversion-basics/SLC_51ug-m3.jpg" width="99%"></p>
</section>
<section id="get-the-data" class="level2">
<h2>Get the data</h2>
<p>The data we’ll be using for the analysis comes from the <a href="https://deq.utah.gov/Divisions/daq/index.htm">UDAQ</a> via the EPA, which keeps detailed air quality data for various sites in SL County. This post will grab data from Hawthorne Elementary (site 490353006) in SLC proper, as that site is noted as being the <a href="http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc">most accurate</a> in the valley (and is <a href="http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc">actually the site used</a> to determine whether the valley meets EPA standards).</p>
<p>We’ll pull 2017 hourly data from <a href="https://aqs.epa.gov/api">here</a> (you’ll have to register) with these parameters:</p>
<p><img src="figs/2017-01-11-inversion-basics/data_download_parameters.png" width="99%"></p>
</section>
<section id="analysis" class="level2">
<h2>Analysis</h2>
<p>We’ll be working with R and the <a href="https://www.tidyverse.org/">tidyverse</a>.</p>
<section id="load-in-the-data" class="level3">
<h3>Load in the data</h3>
<pre class="r"><code>library(tidyverse)
df &lt;- read_csv(&#39;../data/SLCHawthornePM2-5_2013-2017.csv&#39;)

# Look at the data
print(df, n = 5)</code></pre>
<pre><code>## # A tibble: 59,736 x 25
##   Latit… Longi… Datum `Hor… `Sta… `Cou… `Sit… `Par…   POC `AQS… `Date Loc…
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt;    
## 1   40.7   -112 WGS84  24.3    49 035    3006 88101     1 PM2.… 2013-03-20
## 2   40.7   -112 WGS84  24.3    49 035    3006 88101     1 PM2.… 2013-04-13
## 3   40.7   -112 WGS84  24.3    49 035    3006 88101     1 PM2.… 2013-03-21
## 4   40.7   -112 WGS84  24.3    49 035    3006 88101     1 PM2.… 2013-04-30
## 5   40.7   -112 WGS84  24.3    49 035    3006 88101     1 PM2.… 2013-02-05
## # ... with 5.973e+04 more rows, and 14 more variables: `24 Hour Local`
## #   &lt;time&gt;, `Date GMT` &lt;date&gt;, `24 Hour GMT` &lt;time&gt;, `Year GMT` &lt;int&gt;,
## #   `Day In Year GMT` &lt;int&gt;, `Sample Measurement` &lt;dbl&gt;, `Units of
## #   Measure` &lt;chr&gt;, `Sample Duration` &lt;chr&gt;, `Sample Frequency` &lt;chr&gt;,
## #   `Detection Limit` &lt;int&gt;, `Measurement Uncertainty` &lt;chr&gt;, `Qualifier
## #   Description` &lt;chr&gt;, `Method Type` &lt;chr&gt;, `Method Description` &lt;chr&gt;</code></pre>
<p>Around 60k observations for the five year period and lots of columns we don’t need. Let’s simplify this dataset and look again.</p>
<pre class="r"><code># Convert date and time columns into one column
df$DateTime &lt;- as.POSIXct(paste(df$`Date Local`, df$`24 Hour Local`), 
                          format=&quot;%Y-%m-%d %H:%M&quot;)

# Rename column to avoid spaces
df &lt;- rename(df, Measurement = `Sample Measurement`)

# Grab only 1-hr readings, remove readings of PM2.5 below 0
df_slim_hourly &lt;- df %&gt;%
  filter(`Sample Duration` == &#39;1 HOUR&#39; &amp; Measurement &gt;= 0) %&gt;%
  arrange(DateTime) %&gt;%
  select(DateTime, Measurement) # Grab only necessary columnns

print(df_slim_hourly, n = 5)</code></pre>
<pre><code>## # A tibble: 57,483 x 2
##   DateTime            Measurement
##   &lt;dttm&gt;                    &lt;dbl&gt;
## 1 2013-01-01 00:00:00        27.7
## 2 2013-01-01 01:00:00        19.2
## 3 2013-01-01 02:00:00        18.9
## 4 2013-01-01 03:00:00        18.5
## 5 2013-01-01 04:00:00        26.7
## # ... with 5.748e+04 more rows</code></pre>
<p>Fairly simple time series setup here–nice. But more cleaning is necessary. To further simplify the analyis, let’s combine measurements from multiple intra-hour readings at the Hawthorne site, such that there’s just one per-hour.</p>
<pre class="r"><code>df_slim_hourly &lt;- df_slim_hourly %&gt;% 
  na.omit(Measurement) %&gt;% 
  group_by(DateTime) %&gt;% 
  summarise(Measurement = mean(Measurement))</code></pre>
<p>That takes us down to ~42k rows over five years.</p>
</section>
<section id="time-series-for-2013-2017" class="level3">
<h3>Time series for 2013-2017</h3>
<p>Now let’s take a look at the five year time series. We’ll use a log y-scale, such that the high-pollution outlier days don’t dominate the visualization.</p>
<pre class="r"><code>library(ggplot2)
library(scales)
ggplot(data = df_slim_hourly, aes(x = DateTime, y = Measurement)) +
  geom_point() +
  labs(title = &quot;Hourly PM2.5 in SLC from 2013 - 2017&quot;) +
  scale_y_log10(name = &quot;PM2.5 µg/m3&quot;, 
                limits = c(1,NA), 
                breaks = c(1,20,40,60,80,100)) +
  scale_x_datetime(name = &quot;Year&quot;, 
                   labels=date_format(&quot;%m-%Y&quot;), 
                   breaks = date_breaks(&quot;1 year&quot;))</code></pre>
<p><img src="/blog/2018-01-11-inversion-basics2017_files/figure-html/unnamed-chunk-4-1.png" width="650px" /></p>
<p>This is daily data, so it’s messy, but it <em>seems</em> that the winter peaks are becoming less peak-y. Hmm. Let’s try to nail down the trend with a little more rigor.</p>
</section>
<section id="periodicity-in-the-data" class="level3">
<h3>Periodicity in the data</h3>
<p>To understand the trend, we first have to grapple with seasonality. To find seasonality (or cyclic patterns in general), the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">fast fourier transform</a> is a fantastic tool that essentially turns a time domain into a frequency domain. There are several ways to do it–we’ll keep it simple via the r stats package.</p>
<p>First, we remove any nulls and pull the measurement vector from the tibble. Note that because Nov 2013 is missing hourly data, <em>we’ll use data from 2014-2017 for this section</em>.</p>
<pre class="r"><code># Order sample vector by date
measurement_vect_hourly &lt;- df_slim_hourly %&gt;% 
  filter(DateTime &gt; &#39;2013-12-31&#39;) %&gt;% 
  arrange(DateTime) %&gt;% 
  pull(Measurement)</code></pre>
<p>Now we compute the spectral density and switch from frequency to periods (borrowing from <a href="https://rstudio-pubs-static.s3.amazonaws.com/9428_1197bd003ebd43c49b429f22ea4f36e5.html">here</a>)</p>
<pre class="r"><code>spectral_data &lt;- stats::spec.pgram(measurement_vect_hourly, plot = FALSE)

# Grab what we need from the raw output
spec.df &lt;- as.tibble(list(freq = spectral_data$freq, 
                          spec = spectral_data$spec))

# Create a vector of periods to label on the graph, units are in days
days.period &lt;- rev(c(1/4, 1/2, 1, 7, 30, 90, 180, 365))
days.labels &lt;- rev(c(&quot;1/4&quot;, &quot;1/2&quot;, &quot;1&quot;, &quot;7&quot;, &quot;30&quot;, &quot;90&quot;, &quot;180&quot;, &quot;365&quot;))
# Convert daily period to daily freq, and then to hourly freq
days.freqs &lt;- 1/days.period * 1/24</code></pre>
<p>Let’s plot the spectral density via a smoothed periodogram, which shows the frequency of reoccuring patterns. Quick, before looking, think of what cycles you’d expect to see!</p>
<pre class="r"><code>ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() + 
  labs(title = &quot;PM2.5 Power Spectral Density 2014-2017&quot;) +
  scale_x_continuous(name = &quot;Period (days)&quot;, 
                     breaks = days.freqs, 
                     labels = days.labels) +
  scale_y_continuous(name = &quot;Power spectral density&quot;)</code></pre>
<p><img src="/blog/2018-01-11-inversion-basics2017_files/figure-html/unnamed-chunk-7-1.png" width="650px" /></p>
<p>First, note that the X-axis starts at 365 days. While it’s a bit messy, we see diurnal (i.e., daily) and minimal half-day patterns emerging. This would align with intuition, as factories, furnaces, and vehicle traffic would certainly ramp up roughly on a daily cycle.</p>
<p>The half-day emissions cycle also makes some sense, as there are roughly two intuitive times each day when PM2.5 would spike–when factories/furnances ramp up along with the morning commute, and a discrete spike matching the evening commute.</p>
<p>Note the ~yearly pattern is pronounced, and not just occurring on a strict 365-day cycle, which makes sense. Now let’s use log-10 axes to see if the plot is easier to read.</p>
<pre class="r"><code>ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() +
  labs(title = &quot;PM2.5 Power Spectral Density log10-log10 2014-2017&quot;) +
  scale_x_log10(&quot;Period (days)&quot;, breaks = days.freqs, labels = days.labels) + 
  scale_y_log10(&quot;Power spectral density&quot;)</code></pre>
<p><img src="/blog/2018-01-11-inversion-basics2017_files/figure-html/unnamed-chunk-8-1.png" width="650px" /> While it’s certainly a bit messy, the half and full day cycle are still pronounced, while the one-year cycle is harder to pick out. There is some sense to this, as the winter inversions certainly aren’t consistent in their arrival or duration.</p>
</section>
<section id="breaking-out-the-trend-and-the-seasonality" class="level3">
<h3>Breaking out the trend and the seasonality</h3>
<p>Now let’s think about <em>removing</em> the seasonality from the data, such that we can get at the trend. Fortunately, this is still straightforward in R.</p>
<p>To simplify the analysis, let’s go back and grab the daily data (instead of hourly) <em>from 2013-2017</em></p>
<pre class="r"><code>df_slim_daily &lt;- df %&gt;%
  filter(`Sample Duration` == &#39;24 HOUR&#39; &amp; Measurement &gt;= 0) %&gt;%
  mutate(DateTime = as.Date(DateTime)) %&gt;% # Switch to Date
  rename(Date = DateTime) %&gt;%              # Remove time column
  arrange(Date) %&gt;%
  select(Date, Measurement)                # Grab necessary columns

# Prepare vector for seasonal decomposition
measurement_vect_daily &lt;- df_slim_daily %&gt;% 
  arrange(Date) %&gt;%
  pull(Measurement)</code></pre>
<p>While there are <a href="https://cran.r-project.org/web/views/TimeSeries.html">many ways to do this</a>, we’ll keep it simple and use the stats package’s <code>stl</code> function to separate the seasonal component from the secular trend. See more via <code>?stl</code>.</p>
<pre class="r"><code># Convert measurement vector to time series object
measurement_ts &lt;- ts(measurement_vect_daily, 
                     start=c(2013,1,1), 
                     frequency = 365.25)

# Perform decomposition into trend, seasonal, etc
# Higher values of s.window make seasonal component more static
stl_slim &lt;- stats::stl(x = measurement_ts, 
                       s.window = 15)

# Put trend back into daily tibble
df_slim_daily$Trend &lt;- stl_slim$time.series[,&#39;trend&#39;]

# Show data
head(df_slim_daily, 4)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   Date       Measurement Trend
##   &lt;date&gt;           &lt;dbl&gt; &lt;dbl&gt;
## 1 2013-01-01        23.0  16.2
## 2 2013-01-02        26.5  16.2
## 3 2013-01-03        37.1  16.2
## 4 2013-01-04        41.5  16.1</code></pre>
<p>You see that, besides the <code>Date</code> column we have the raw daily <code>Measurements</code> as well as the secular <code>Trend</code> column. Let’s overlay these two to see what pollution progress (if any) SLC has made over the last five years.</p>
<p>Note that there is missing daily readings from the winter of 2014-2015, so don’t focus on that dip.</p>
<pre class="r"><code># Plot trend over daily data
ggplot(df_slim_daily, aes(x = Date)) +
  geom_line(aes(y = Trend, colour = &quot;Trend&quot;), size = 3) +
  geom_line(aes(y = Measurement, colour = &quot;Daily Data&quot;)) +
  labs(title = &quot;PM2.5 Daily Measurements and Trend in SLC from 2013 - 2017&quot;) +
  scale_colour_manual(values = c(&quot;black&quot;, &quot;red&quot;)) +
  scale_y_log10(name = &quot;PM2.5 µg/m3&quot;, 
                limits = c(1,NA), 
                breaks = c(1,20,40,60,80,100)) +
  scale_x_date(name = &quot;Year&quot;, 
               labels=date_format(&quot;%m-%Y&quot;), 
               breaks = date_breaks(&quot;1 year&quot;))</code></pre>
<p><img src="/blog/2018-01-11-inversion-basics2017_files/figure-html/unnamed-chunk-11-1.png" width="650px" /></p>
</section>
</section>
<section id="conclusions" class="level2">
<h2>Conclusions</h2>
<p>Hooray, perhaps tentative progress! Note that the de-seasonalized daily 2017 PM2.5 values were generally below those of 2013-2014. The improvement appears to be coming from reducing the winter spikes, which is great. While the trend isn’t dramatic and I’d love to corroborate with data from other sites, we do at least appear in a better spot than five years ago.</p>
<p><strong>However</strong>, we still have issues in SLC</p>
<ul>
<li>The downward trend has stalled in 2016-2017; what new policies had started in 2013?</li>
<li>Our <a href="http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc">“best”</a> PM2.5 site has recent missing data for both 1-hr and 24-hr readings–why?</li>
<li>Salt Lake County is still <a href="https://www3.epa.gov/airquality/urbanair/sipstatus/reports/ut_areabypoll.html">classified</a> as a <em>Serious</em> non-attainment area by the EPA.</li>
</ul>
<p>Non-attainment status comes when areas “<a href="https://deq.utah.gov/Pollutants/P/pm/pm25/areas.htm">persistently exceed</a> the National Ambient Air Quality Standards.” Apparently the county moved from <em>moderate</em> to <em>serious</em> non-attainment because it <a href="https://archive.sltrib.com/article.php?id=5240322&amp;itype=CMSID">didn’t meet a 2015 EPA air quality complicance deadline</a>. It’s still unhealthy for folks to spend time outside on certain winter days, and that’s not acceptable.</p>
</section>
<section id="next-steps" class="level2">
<h2>Next steps</h2>
<p>Soon I’ll try to nail down the trend with <em>complete</em> daily data from 2013-2017, provide easier year-over-year comparisons, and make this more robust via a 2008-2017 trend with data from multiple sites.</p>
<p>Note that the 2018 session for the <a href="https://le.utah.gov/Documents/find.htm">Utah Legislature</a> starts on January 22nd!</p>
</section>
