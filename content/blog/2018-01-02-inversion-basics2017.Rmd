---
title: "SLC Air Quality - Five Years On"
excerpt: "The five-year state of the air"
date: '2018-01-02'
draft: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dev = 'png',
                      dpi = 200, out.width='650px', fig.width=8, fig.height=4)
```

## Background on SLC Air

For those unfamiliar, SLC is famous for its winter temperature inversions. These things are famous for trapping emissions and making life unpleasant. There's tons of great background [here](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html) and [here](https://ehp.niehs.nih.gov/1104349/). The focus of _today's_ post will be digging into 2017 values of **PM2.5** in SLC, as this type of pollutant is most frequently linked to [negative health outcomes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4740125/).

![](http://health.utah.gov/utahair/SLC_51ug-m3.jpg)

## Get the data

The data we'll be using for the analysis comes from the [UDAQ](https://deq.utah.gov/Divisions/daq/index.htm) via the EPA, which keeps detailed air quality data for various sites in SL County. This post will grab data from Hawthorne Elementary in SLC proper, as that site is noted as being the [most accurate](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc) in the valley (and is [actually the site used](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc) to determine whether the valley meets EPA standards).

We'll pull 2017 hourly data from [here](https://aqs.epa.gov/api) (you'll have to register) with these parameters:

<img src="figs/2017-01-06-inversion-basics/data_download_parameters.png" width="800">

## Analysis

We'll be working via the [tidyverse](https://www.tidyverse.org/), which means we're using R and playing with [tibbles](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html).

### Load in the data

```{r}
library(tidyverse)
df <- read_csv('../data/SLCHawthornePM2-5_2013-2017.csv')

# Look at the data
print(df, n = 5)
```

Around 60k observations for the five year period and lots of columns we don't need. Let's simplify this dataset and look at it.

```{r}
# Convert date and time columns into one column
df$DateTime <- as.POSIXct(paste(df$`Date Local`, df$`24 Hour Local`), 
                          format="%Y-%m-%d %H:%M")

# Rename column to avoid spaces
df <- rename(df, Measurement = `Sample Measurement`)

# Grab only 1-hr readings and grab only needed columns
df_slim <- df %>%
  filter(`Sample Duration` == '1 HOUR') %>% 
  select(DateTime, Measurement)

print(df_slim, n = 5)
```
Combine measurements from multiple sensors, such that there's just one per-hour.

```{r}
df_slim <- df_slim %>% 
  group_by(DateTime) %>% 
  summarise(Measurement = mean(Measurement))
```
That takes us down to ~42k rows over five years.

### Time series for 2013-2017

Now let's take a look at the five year time series. We'll use a log y-scale, such that the high-pollution outlier days don't dominate the visualization.

```{r, echo=TRUE}
library(ggplot2)
library(scales)
ggplot(data = df_slim, aes(x = DateTime, y = Measurement)) +
  geom_point() +
  #stat_smooth(method = 'lm') +
  labs(title = "PM2.5 in SLC from 2013 - 2017", x = "Year") +
  scale_y_log10(limits = c(1,NA), breaks = c(1,20,40,60,80,100)) +
  scale_x_datetime( labels=date_format("%m-%Y"), breaks = date_breaks("1 year"))
```

Now it _seems_ that the winter peaks are becoming less peak-y, but how do we nail down the trend with rigor?

### Periodicity in the data

To find the seasonality in data, the fast fourier transform[https://en.wikipedia.org/wiki/Fast_Fourier_transform] is a fantastic tool. It essential turns a time domain into a frequency domain and helps you understand the reoccuring patterns in the data.

First, we remove any nulls and pull the measurement vector from the tibble

```{r}
# Order sample vector by date
measurement_vect <- df_slim %>% 
  na.omit() %>%
  arrange(DateTime) %>% 
  pull(Measurement)
```

Now we compute the spectral density and switch from frequency to periods (borrowing from [here](https://rstudio-pubs-static.s3.amazonaws.com/9428_1197bd003ebd43c49b429f22ea4f36e5.html))

```{r}
spectral_data <- stats::spec.pgram(measurement_vect, plot = FALSE)

# Grab what we need from the raw output
spec.df <- as.tibble(list(freq = spectral_data$freq, spec = spectral_data$spec))

# Create a vector of periods to label on the graph, units are in days
days.period <- rev(c(1/4, 1/2, 1, 7, 30, 90, 180, 365))
days.labels <- rev(c("1/4", "1/2", "1", "7", "30", "90", "180", "365"))
days.freqs <- 1/days.period * 1/24  # Convert daily period to daily freq, and then to hourly freq

```

Let's plot the raw periodogram, which shows the periodicity (or reoccuring patterns).

```{r}
ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() + 
  scale_x_continuous("Period (days)", breaks = days.freqs, labels = days.labels) + 
  scale_y_continuous()
```

While it's a bit messy, we see diurnal (i.e., daily) and half-day patterns emerging. This would align with intuition, as factories, furnaces, and vehicle traffic would certainly ramp up at roughly on a daily cycle. The half-day 

Now let's use log-10 axis to see if the patterns change much
```{r}
ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() + 
  scale_x_log10("Period (days)", breaks = days.freqs, labels = days.labels) + 
  scale_y_log10()
```
While it's certainly a bit messy, we see that there are 

### Breaking out the trend and the seasonality



## Conclusions and next steps


Note: Hawthorne Elementary is site 490353006

