---
title: "SLC Air Quality - Five Years On"
excerpt: "Are we doing better or worse?"
date: '2018-01-02'
draft: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dev = 'png',
                      dpi = 200, out.width='650px', fig.width=8, fig.height=4)
```

## Background on SLC Air

For those unfamiliar, SLC is famous for its winter temperature inversions. These things are famous for trapping emissions and making life unpleasant. There's tons of great background [here](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html) and [here](https://ehp.niehs.nih.gov/1104349/). The focus of _today's_ post will be digging into 2017 values of **PM2.5** in SLC, as this type of pollutant is most frequently linked to [negative health outcomes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4740125/).

![](http://health.utah.gov/utahair/SLC_51ug-m3.jpg)

## Get the data

The data we'll be using for the analysis comes from the [UDAQ](https://deq.utah.gov/Divisions/daq/index.htm) via the EPA, which keeps detailed air quality data for various sites in SL County. This post will grab data from Hawthorne Elementary (site 490353006) in SLC proper, as that site is noted as being the [most accurate](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc) in the valley (and is [actually the site used](http://home.chpc.utah.edu/~whiteman/PM2.5/PM2.5.html#current_conc) to determine whether the valley meets EPA standards).

We'll pull 2017 hourly data from [here](https://aqs.epa.gov/api) (you'll have to register) with these parameters:

<img src="figs/2017-01-06-inversion-basics/data_download_parameters.png" width="800">

## Analysis

We'll be working via the [tidyverse](https://www.tidyverse.org/), which means we're using R and playing with [tibbles](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html).

### Load in the data

```{r}
library(tidyverse)
df <- read_csv('../data/SLCHawthornePM2-5_2013-2017.csv')

# Look at the data
print(df, n = 5)
```

Around 60k observations for the five year period and lots of columns we don't need. Let's simplify this dataset and look at it.

```{r}
# Convert date and time columns into one column
df$DateTime <- as.POSIXct(paste(df$`Date Local`, df$`24 Hour Local`), 
                          format="%Y-%m-%d %H:%M")

# Rename column to avoid spaces
df <- rename(df, Measurement = `Sample Measurement`)

# Grab only 1-hr readings, remove readings of PM2.5 below 0, and grab only needed columns
df_slim <- df %>%
  filter(`Sample Duration` == '1 HOUR' & Measurement >= 0) %>% 
  select(DateTime, Measurement)

print(df_slim, n = 5)
```
Combine measurements from multiple sensors, such that there's just one per-hour.

```{r}
df_slim <- df_slim %>% 
  group_by(DateTime) %>% 
  summarise(Measurement = mean(Measurement))
```
That takes us down to ~42k rows over five years.

### Time series for 2013-2017

Now let's take a look at the five year time series. We'll use a log y-scale, such that the high-pollution outlier days don't dominate the visualization.

```{r, echo=TRUE}
library(ggplot2)
library(scales)
ggplot(data = df_slim, aes(x = DateTime, y = Measurement)) +
  geom_point() +
  labs(title = "PM2.5 in SLC from 2013 - 2017") +
  scale_y_log10(name = "PM2.5 µg/m3", limits = c(1,NA), breaks = c(1,20,40,60,80,100)) +
  scale_x_datetime(name = "Year", labels=date_format("%m-%Y"), breaks = date_breaks("1 year"))
```

Now it _seems_ that the winter peaks are becoming less peak-y, but how do we nail down the trend with rigor?

### Periodicity in the data

To find the seasonality in data, the fast fourier transform[https://en.wikipedia.org/wiki/Fast_Fourier_transform] is a fantastic tool. It essential turns a time domain into a frequency domain and helps you understand the reoccuring patterns in the data.

First, we remove any nulls and pull the measurement vector from the tibble

```{r}
# Order sample vector by date
measurement_vect <- df_slim %>% 
  na.omit() %>%
  arrange(DateTime) %>% 
  pull(Measurement)
```

Now we compute the spectral density and switch from frequency to periods (borrowing from [here](https://rstudio-pubs-static.s3.amazonaws.com/9428_1197bd003ebd43c49b429f22ea4f36e5.html))

```{r}
spectral_data <- stats::spec.pgram(measurement_vect, plot = FALSE)

# Grab what we need from the raw output
spec.df <- as.tibble(list(freq = spectral_data$freq, spec = spectral_data$spec))

# Create a vector of periods to label on the graph, units are in days
days.period <- rev(c(1/4, 1/2, 1, 7, 30, 90, 180, 365))
days.labels <- rev(c("1/4", "1/2", "1", "7", "30", "90", "180", "365"))
days.freqs <- 1/days.period * 1/24  # Convert daily period to daily freq, and then to hourly freq

```

Let's plot the raw periodogram (or power spectral density), which shows the frequency of reoccuring patterns. Quick, before looking, think of what cycles you'd expect to see!

```{r}
ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() + 
  labs(title = "PM2.5 Power Spectral Density 2013-2017") +
  scale_x_continuous(name = "Period (days)", breaks = days.freqs, labels = days.labels, limits = c(NA, 0.1)) + 
  scale_y_continuous(name = "Power spectral density")
```

While it's a bit messy, we see diurnal (i.e., daily) and minimal half-day patterns emerging. This would align with intuition, as factories, furnaces, and vehicle traffic would certainly ramp up roughly on a daily cycle. 

The half-day emissions cycle also makes some sense, as there are roughly two intuitive times each day when PM2.5 would spike--when factories/furnances ramp up along with the morning commute, and a discrete spike matching the evening commute.

Now let's use log-10 axes to see if the patterns change much
```{r}
ggplot(data = spec.df, aes(x = freq, y = spec)) + 
  geom_line() +
  labs(title = "PM2.5 Power Spectral Density log10-log10 2013-2017") +
  scale_x_log10("Period (days)", breaks = days.freqs, labels = days.labels) + 
  scale_y_log10("Power spectral density")
```
This is your classic spectral density plot. While it's certainly a bit messy, the half and full day cycle are corroborated here, while the one-year cycle is harder to pick out. There is some sense to this, as the winter inversions certainly aren't consistent in their arrival or duration.

### Breaking out the trend and the seasonality

This seasonality makes trend analysis a bit more involved, as one has to first remove the seasonal effect. Fortunately, this is still straightforward in R. While there are [many ways to do this](https://cran.r-project.org/web/views/TimeSeries.html), we'll keep it simple and use the stats package's `stl` function. See more via `?stl`.

```{r}
library(ggseas)
ggplot(data = df_slim, aes(x = DateTime, y = Measurement)) +
  geom_point() +
  stat_seas(color = "red", size = 3) +
  #labs(title = "PM2.5 in SLC from 2013 - 2017") +
  scale_y_log10(name = "PM2.5 µg/m3", limits = c(1,NA), breaks = c(1,20,40,60,80,100))
  #scale_x_datetime(name = "Year", labels=date_format("%m-%Y"), breaks = date_breaks("1 year"))

# ggsdc(data = df_slim, aes(x = DateTime, y = Measurement), method = "seas") + 
#   geom_line()
```




```{r}
# Convert measurement vector to time series object

#Pull in 12/17 data!
#measurement_ts <- ts(measurement_vect, start=c(2013,1), frequency = 8760)

# Perform decomposition into parts via additive component
#decomposed_slim <- stats::stl(x = measurement_ts, s.window = "periodic")
#plot(decomposed_slim)
# Plot

```


## Conclusions and next steps



